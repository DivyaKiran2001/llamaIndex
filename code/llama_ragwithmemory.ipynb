{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divyakirant\\Documents\\llamaIndex\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_20664\\2549428705.py:6: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_20664\\2549428705.py:7: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,Settings\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "\n",
    "# Configure Gemini models\n",
    "gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
    "gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n",
    "\n",
    "# Tell LlamaIndex to use Gemini everywhere\n",
    "Settings.llm = gemini_llm\n",
    "Settings.embed_model = gemini_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatSummaryMemoryBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ChatSummaryMemoryBuffer.from_defaults(token_limit=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_files=['./data/ragintro.pdf'])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SentenceSplitter(chunk_size=100, chunk_overlap=20)\n",
    "all_documents = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors Store: <llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x0000021EA490AF90>\n",
      "Vectors Store ID: 47bc7442-3490-4f9d-8257-1e1534649987\n"
     ]
    }
   ],
   "source": [
    "# store the documents in a vector store index\n",
    "index = VectorStoreIndex.from_documents(documents=all_documents)\n",
    "\n",
    "print(f\"Vectors Store: {index}\")\n",
    "print(f\"Vectors Store ID: {index.index_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine=index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    llm=gemini_llm,\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an pdf loaded in this context.If the question is not relevant to the context and in the loaded documents, just say I don't know.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I'm ready to chat and answer your questions about the provided PDF excerpts.  Specifically, I can tell you about different chunking strategies, particularly regarding file format-based chunking.  For example, I know that it's recommended to keep HTML elements like tables and images intact during chunking, and similar considerations should be applied to PDFs.  I also know about the key RAG stages: Indexing, Retrieval, and Augmentation.  I even have some page number references for this information.\n",
      "\n",
      "What can I tell you about chunking or RAG?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"various strategies of chunking from the pdf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chat History:\n",
      "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='various strategies of chunking from the pdf')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=\"Okay, I'm ready to chat and answer your questions about the provided PDF excerpts.  Specifically, I can tell you about different chunking strategies, particularly regarding file format-based chunking.  For example, I know that it's recommended to keep HTML elements like tables and images intact during chunking, and similar considerations should be applied to PDFs.  I also know about the key RAG stages: Indexing, Retrieval, and Augmentation.  I even have some page number references for this information.\\n\\nWhat can I tell you about chunking or RAG?\\n\")])]\n"
     ]
    }
   ],
   "source": [
    "# print the chat history\n",
    "print(\"\\nChat History:\")\n",
    "print(memory.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Based on the context you provided, I can elaborate on a few points related to chunking and Retrieval Augmented Generation (RAG):\n",
      "\n",
      "* **Chunking in the context of PDFs:**  While the provided text doesn't explicitly detail *specific* chunking strategies for PDFs, it implies the importance of thoughtful chunking, similar to HTML.  This suggests that preserving the logical structure of the PDF is crucial.  Think about how a PDF might be organized: sections, subsections, figures, tables, etc.  Ideal chunking would likely keep these elements together rather than splitting them arbitrarily.  For example, a whole table should be in one chunk, not split across multiple chunks.  This is important for maintaining context and ensuring that the retrieved information is coherent.\n",
      "\n",
      "* **Connection to RAG:**  Chunking is the first step in the \"Indexing\" stage of RAG.  The PDF is broken down into smaller, manageable pieces (chunks) that can then be indexed by a search engine.  This allows the system to quickly retrieve relevant information later during the \"Retrieval\" stage.  The quality of the chunks directly impacts the effectiveness of retrieval.  Poorly chunked data can lead to irrelevant or incomplete results.\n",
      "\n",
      "* **Importance of Context:** The provided DOIs and retrieval dates suggest academic papers related to RAG or natural language processing.  This implies a focus on semantic understanding and context preservation, which reinforces the importance of intelligent chunking.\n",
      "\n",
      "* **Further Information:**  While the provided context gives hints, it doesn't contain detailed explanations of various chunking strategies.  To get more specific information, you'd need to consult the actual papers referenced by the DOIs.  They likely contain more in-depth discussions of chunking techniques.\n",
      "\n",
      "Is there anything specific you'd like to know more about?  For example, are you interested in specific chunking algorithms, or perhaps the other stages of RAG (Retrieval and Augmentation)?  Knowing what you're most interested in will help me provide more relevant information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"can you explain more \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
