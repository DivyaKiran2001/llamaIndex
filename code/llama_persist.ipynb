{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_3348\\2579795777.py:6: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Doc ID: 48fc1686-65f2-47cd-aa0d-d219be48b29f\n",
      "Text: this is a sample text file to test persistence in llamaindex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_3348\\2579795777.py:7: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,Settings\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "\n",
    "# Configure Gemini models\n",
    "gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
    "gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n",
    "\n",
    "# Tell LlamaIndex to use Gemini everywhere\n",
    "Settings.llm = gemini_llm\n",
    "Settings.embed_model = gemini_embedding\n",
    "\n",
    "documents=SimpleDirectoryReader(input_dir=\"data\",required_exts=[\".txt\"]).load_data()\n",
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=VectorStoreIndex.from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage context\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "index.set_index_id(\"vector_index\")\n",
    "index.storage_context.persist(\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 12:59:45,991 - INFO - Loading indices with ids: ['vector_index']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "# load index\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "#index.set_index_id(\"vector_index\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index=load_index_from_storage(storage_context,index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the text file mentions testing persistence in llamaindex.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"do i have llamaindex in this text file\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
