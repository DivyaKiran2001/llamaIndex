{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divyakirant\\Documents\\llamaIndex\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_5504\\4178048077.py:1: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  llm=Gemini(\n"
     ]
    }
   ],
   "source": [
    "llm=Gemini(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot give you live, real-time weather updates for Hyderabad.  I'm a language model, not connected to live weather data. \n",
      "\n",
      "To get the current weather in Hyderabad, I recommend checking a reliable weather app or website such as:\n",
      "\n",
      "* **Google Weather:** Just search \"weather in Hyderabad\" on Google.\n",
      "* **AccuWeather:**  Accuweather.com\n",
      "* **Weather.com (The Weather Channel):** weather.com\n",
      "* **Local news websites or apps:** Many local news sources in Hyderabad will have up-to-the-minute weather information.\n",
      "\n",
      "\n",
      "These resources will give you the most accurate and current conditions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=llm.complete(\"How is the weather now in hyderabad?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex (formerly GPT Index) is a data framework that allows you to connect your external data to large language models (LLMs).  It helps you structure your data in a way that LLMs can understand and use effectively, enabling more powerful and context-aware interactions.\n",
      "\n",
      "Here's a breakdown of its key features and benefits:\n",
      "\n",
      "* **Data Connection:** LlamaIndex can connect to a wide variety of data sources, including:\n",
      "    * APIs\n",
      "    * PDFs\n",
      "    * Documents (Word, txt, etc.)\n",
      "    * Databases\n",
      "    * Code repositories\n",
      "    * Cloud storage (e.g., S3, Google Cloud Storage)\n",
      "    * Private websites (via web scraping)\n",
      "\n",
      "* **Data Indexing:**  It structures your data into different index types optimized for various LLM tasks. These indexes represent different ways of organizing and storing the information, making it easier for the LLM to access and process.  Examples include:\n",
      "    * **List Index:**  Simple list of text chunks. Good for summarization.\n",
      "    * **Tree Index:** Hierarchical structure for navigating complex documents.\n",
      "    * **Vector Store Index:**  Uses embeddings to find semantically similar chunks of text.  Good for question answering.\n",
      "    * **Keyword Table Index:**  Uses keywords to retrieve relevant information.\n",
      "\n",
      "* **Querying:** LlamaIndex provides a simple interface for querying your data through the LLM. You can ask questions, request summaries, or perform other tasks based on the information stored in the indexes.\n",
      "\n",
      "* **Composability:** You can combine different indexes to create more complex and powerful applications.\n",
      "\n",
      "* **Customization:** LlamaIndex offers a high degree of customization, allowing you to tailor the indexing and querying process to your specific needs. You can choose different LLMs, embedding models, and other parameters.\n",
      "\n",
      "* **Reduced Hallucinations:** By grounding the LLM's responses in your data, LlamaIndex helps to minimize hallucinations (incorrect or fabricated information).\n",
      "\n",
      "* **Cost Savings:** By only sending relevant data chunks to the LLM, LlamaIndex can help reduce the cost and latency associated with LLM API calls, especially for large datasets.\n",
      "\n",
      "\n",
      "**In essence, LlamaIndex acts as a bridge between your data and the LLM, enabling the LLM to leverage the information in your data to generate more accurate, relevant, and insightful responses.**  It's a valuable tool for building applications that require LLMs to interact with external data sources.\n"
     ]
    }
   ],
   "source": [
    "response = llm.stream_complete(\"What is the llama-index?\")\n",
    "for chunk in response:\n",
    "    print(chunk.delta, end='') # try excluding delta parameter to see the full response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "messages=[\n",
    "    ChatMessage(\n",
    "        role=\"system\",content=\"you are an Content writer for writing content for content creation\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\",content=\"help me to write about llamaindex and how it is different from other frameworks \")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: ## LlamaIndex: Unlocking the Power of Your Data for LLMs\n",
      "\n",
      "Large Language Models (LLMs) are revolutionizing how we interact with information.  However, their knowledge is limited to the data they were trained on.  What if you want to use an LLM with your *own* data, like private documents, internal wikis, or a vast codebase?  That's where LlamaIndex comes in.\n",
      "\n",
      "LlamaIndex (formerly GPT Index) is a data framework that bridges the gap between your data and LLMs.  It allows you to connect your data to LLMs, enabling them to answer questions, generate text, and perform other tasks based on the information within your dataset.  This opens up a world of possibilities, from building custom chatbots trained on your company's documentation to generating personalized learning materials from research papers.\n",
      "\n",
      "**How is LlamaIndex different from other frameworks?**\n",
      "\n",
      "While several tools aim to connect LLMs with external data, LlamaIndex stands out in several key ways:\n",
      "\n",
      "* **Simplified Data Connection:** LlamaIndex offers a streamlined process for connecting to various data sources.  Whether it's a PDF, a CSV file, a Notion database, or even a code repository, LlamaIndex provides pre-built connectors and data loaders to simplify ingestion.  You don't need to be a data engineering expert to get started.\n",
      "\n",
      "* **Advanced Indexing and Structuring:**  LlamaIndex goes beyond simple keyword search. It employs advanced indexing techniques, including vector databases and tree-based structures, to organize and represent your data effectively. This allows LLMs to access and understand the context within your data, leading to more accurate and relevant responses.  Other frameworks might rely on simpler embedding methods, which can struggle with larger or more complex datasets.\n",
      "\n",
      "* **Querying Flexibility:** LlamaIndex offers a variety of query interfaces, allowing you to interact with your data in different ways.  You can ask questions in natural language, provide specific keywords, or even use structured queries. This flexibility empowers you to tailor your interactions to your specific needs.\n",
      "\n",
      "* **Composability and Customization:** LlamaIndex is designed with modularity in mind.  You can easily customize the indexing process, choose different data structures, and combine various components to create a tailored solution for your specific use case.  This level of customization is often lacking in more rigid frameworks.\n",
      "\n",
      "* **Focus on Retrieval Augmented Generation (RAG):** LlamaIndex is specifically designed to facilitate RAG, a powerful technique that enhances LLM responses by retrieving relevant information from external sources.  This focus ensures that the framework is optimized for tasks that require grounding in specific data, unlike general-purpose LLM tooling.\n",
      "\n",
      "* **Open-Source and Community Driven:** LlamaIndex is an open-source project with a vibrant community. This fosters collaboration, rapid development, and access to a wealth of resources and support.\n",
      "\n",
      "**Beyond the Basics:**\n",
      "\n",
      "LlamaIndex also offers advanced features like:\n",
      "\n",
      "* **Data Augmentation:**  Improve LLM performance by augmenting your data with external knowledge.\n",
      "* **Composite Indexes:** Combine different indexes to create a more comprehensive view of your data.\n",
      "* **Custom Data Structures:** Implement your own data structures to optimize for specific use cases.\n",
      "\n",
      "**In Conclusion:**\n",
      "\n",
      "LlamaIndex empowers you to unlock the full potential of LLMs by connecting them to your own data.  Its simplified data connection, advanced indexing, querying flexibility, and focus on RAG make it a powerful and versatile tool for anyone looking to leverage the power of LLMs in their projects.  Whether you're building a chatbot, generating personalized content, or analyzing complex data, LlamaIndex provides the framework you need to bridge the gap between your data and the transformative capabilities of LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages2 = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"you are a helpful assistant that answers questions about birds.\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: That's a bit like asking \"What's the food name?\"  There are *lots* of colorful birds!  To help me give you a useful answer, could you be more specific?  For example, do you know:\n",
      "\n",
      "* **Where in the world you saw the bird?** (Country, state, region)\n",
      "* **What size was it?** (Sparrow-sized, robin-sized, crow-sized, larger?)\n",
      "* **What were the specific colors?** (e.g., red breast, blue head, yellow wings)\n",
      "* **What was its habitat?** (Forest, field, near water, city park?)\n",
      "\n",
      "The more details you can give me, the better chance I have of identifying the bird for you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.chat(messages2 + [ChatMessage(role=\"user\", content=\"What is the colorful bird name?\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
