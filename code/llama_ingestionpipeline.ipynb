{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divyakirant\\Documents\\llamaIndex\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_3252\\2549428705.py:6: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_3252\\2549428705.py:7: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,Settings\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "\n",
    "# Configure Gemini models\n",
    "gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
    "gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n",
    "\n",
    "# Tell LlamaIndex to use Gemini everywhere\n",
    "Settings.llm = gemini_llm\n",
    "Settings.embed_model = gemini_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:[Document(id_='32b913be-539c-4e5d-9bda-7b6a13fe3206', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\divyakirant\\\\Documents\\\\llamaIndex\\\\data\\\\sample.txt', 'file_name': 'sample.txt', 'file_type': 'text/plain', 'file_size': 1134, 'creation_date': '2025-08-30', 'last_modified_date': '2025-08-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Memory in LlamaIndex – Key Concepts\\r\\n\\r\\nPurpose of Memory\\r\\n\\r\\nMakes interactions stateful (remembers past turns).\\r\\n\\r\\nWithout memory → each query is independent.\\r\\n\\r\\nWith memory → conversation has continuity.\\r\\n\\r\\nHow Memory Works\\r\\n\\r\\nStores chat history (messages, queries, responses).\\r\\n\\r\\nInjects relevant history back into prompts for the LLM.\\r\\n\\r\\nCan store raw text or compressed summaries.\\r\\n\\r\\nMain Types of Memory\\r\\n\\r\\nChatMemoryBuffer → keeps a sliding window of recent chat messages.\\r\\n\\r\\nSummaryMemoryBuffer → compresses old history into summaries to save tokens.\\r\\n\\r\\nSimpleMemory → key-value store for facts (e.g., “User’s name is Divya”).\\r\\n\\r\\nWhy Memory is Needed\\r\\n\\r\\nProvides context continuity (e.g., resolving pronouns like he, it, they).\\r\\n\\r\\nEnables personalization (remembering preferences/facts about the user).\\r\\n\\r\\nHandles long conversations efficiently by summarizing.\\r\\n\\r\\nPrevents token limit overflow by managing how much history is kept.\\r\\n\\r\\nMemory + Query/Chat Engines\\r\\n\\r\\nIn query engines, memory helps refine queries across turns.\\r\\n\\r\\nIn chat engines, memory makes conversations flow naturally like human dialogue.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "reader=SimpleDirectoryReader(input_dir=\"data\",required_exts=[\".txt\"])\n",
    "documents=reader.load_data()\n",
    "print(f\"Documents:{documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"llndex\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"llndex-cjawalj.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 768,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.create_index(name=\"llndex\",dimension=768,spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index=pc.Index(\"llndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "storage_context=StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index=VectorStoreIndex.from_documents(documents,storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_3252\\2139447779.py:5: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  GeminiEmbedding()\n"
     ]
    }
   ],
   "source": [
    "pipeline=IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25,chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        GeminiEmbedding()\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 1}},\n",
       " 'total_vector_count': 1,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(documents)\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All vector IDs: [['32b913be-539c-4e5d-9bda-7b6a13fe3206#82ee5742-f752-480e-a3e4-597f01992695']]\n"
     ]
    }
   ],
   "source": [
    "all_ids=list(pinecone_index.list())\n",
    "print(\"All vector IDs:\",all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:Yes, the user's name is Divya.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_engine=index.as_query_engine()\n",
    "response=query_engine.query(\"do i have divya in the text file?\")\n",
    "print(f\"Response:{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
