LlamaIndex is the framework for Context-Augmented LLM Applications by Jerry Liu
LlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, agents, and more.
It just makes using them easier and provide tools like:
    - Data connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.
    - Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.
    - Engines provide natural language access to your data. For example:
        - Query engines are powerful interfaces for question-answering (e.g. a RAG flow).
        - Chat engines are conversational interfaces for multi-message, "back and forth" interactions with your data.
        - Agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.
        - Observability/Evaluation integrations that enable you to rigorously experiment, evaluate, 
          and monitor your app in a virtuous cycle.
        - Workflows allow you to combine all of the above into an event-driven system far more flexible than other, graph-based approaches.
Source: https://docs.llamaindex.ai/en/stable/

Version during demos preparation of LlamaIndex:  0.13.1

Google/Gemini API key: Google AI Studio
https://aistudio.google.com/

Install using pip:
----------------------
pip install llama-index


LlamaIndex requires - Core libs
---------------------
LllamaIndex - llama-index - V0.3
LlamaIndex requires - llama-index-cli, llama-index-core, llama-index-embeddings-openai, 
llama-index-indices-managed-llama-cloud, llama-index-llms-openai, 
llama-index-readers-file, llama-index-readers-llama-parse, nltk

During installation you will have llama-index-llms-openai, but for Google the install llama-index-llms-google-genai

default model for openai is GPT-3.5-turbo as global settings
you can use the model you want, explicitly configure the LLM using Settings to change the Global configurations

delta parameter - Delta received from the stream chat.

Pinecone Vector store:
https://www.pinecone.io/

Signup -> Create an API Key

metric property for pinecone:
--------------------------------
Documents are converted into vector embeddings (numerical representations of their content), the system needs a way to measure how similar or dissimilar two vectors are. That’s where distance metrics come in.
Euclidean distance is the straight-line distance between two points in space.
In vector search, it’s used to find which document vectors are closest to the query vector—i.e., most relevant.

Readers: SimpleDirectoryReader: Loads files from a directory and converts them into Document objects for ingestion.
Parsing documents into nodes is the process of breaking down larger documents into smaller, structured units called nodes. 
    These nodes are the fundamental building blocks used by LlamaIndex to enable efficient indexing, retrieval, and reasoning over text.
Node: Documents are split into smaller pieces (Nodes or chunks) for efficient processing and retrieval.
VectorStoreIndex: Indexes Nodes and their embeddings in a vector store, enabling semantic search.
Vector store: Pinecone: A managed vector database used to store and search vector embeddings of Nodes.
Types of Splitter: SentenceSplitter: Splits text into sentences or chunks, controlling chunk size and overlap for optimal retrieval.
Indexing: Index: The process of organizing and storing Nodes (chunks) and their embeddings for fast retrieval.
Retriever: VectorIndexRetriever: Fetches the most relevant Nodes from the vector store based on query similarity.
Response Synthesis:Construct a coherent answer based on the nodes,
ResponseSynthesizer: Combines retrieved Nodes and generates a coherent response using an LLM.
responsemode: Determines how responses are synthesized (e.g., "tree_summarize", "compact").
RetrieverQueryEngine: Orchestrates retrieval and response synthesis, providing a unified interface for querying indexed data.

dimensions refers to a configurable parameter that determines the length of the output embedding vector

VectorStoreIndex uses a in-memory SimpleVectorStore that's initialized as part of the default storage context.


Index                              Type	                                   How Nodes Are Retrieved	How They're Synthesized
----------------------------------------------------------------------------------------------------------------------------------
Summary Index     	      Loads all Nodes sequentially	                    Synthesizes a response from the entire list
Vector Store Index	      Fetches top-k similar Nodes via embeddings	      Synthesizes from those top-k Nodes
Tree Index	              Traverses a hierarchy to select Nodes	            Synthesizes from selected leaf Nodes
Keyword Table Index	      Matches query keywords to Node keywords	          Synthesizes from matched Nodes
Property Graph Index	    Retrieves graph triples via keyword &vectorsearch	Synthesizes from triples and optionally source text

Indexing:
  https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide/


Readers:
  SimpleDirectoryReader

Node:
  Chunks

Vector store  
      Pinecone

VectorStoreIndex
Types of Splitter
  SemanticSplitterNodeParser: Splits a document into Nodes, with each node being a group of semantically related sentences.
  SentenceSplitter: Parse text with a preference for complete sentences.
                    In general, this class tries to keep sentences and paragraphs together. 
                    Therefore compared to the original TokenTextSplitter,
                    there are less likely to be hanging sentences or parts of sentences at the end of the node chunk.
  TokenTextSplitter:Implementation of splitting text that looks at word tokens.

Inexing
  Index

Retriever
  VectorIndexRetriever
  ResponseSynthesizer
  responsemode
  RetrieverQueryEngine
  CustomQueryEngine - Not in the scope of the course


All High level steps:
Nodes and Documents: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a "chunk" of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.
Connectors: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.
Indexing Stage#
Indexes: Once you've ingested your data, LlamaIndex will help you index the data into a structure that's easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.
Embeddings: LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.
Querying Stage#
Retrievers: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it's done.
Routers: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate's metadata and the query.
Node Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.
Response Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.

Reference: https://docs.llamaindex.ai/en/stable/understanding/rag/

A Query Engine in LlamaIndex is stateless and designed for single-turn Q&A: it retrieves relevant context 
and synthesizes a response for each query independently.
In contrast, a Chat Engine is stateful, maintaining conversation history 
and context across multiple turns, enabling more natural, multi-turn interactions and follow-up questions.

Chat mode:
best - Turn the query engine into a tool, for use with a ReAct data agent or an OpenAI data agent, depending on what your LLM supports. OpenAI data agents require gpt-3.5-turbo or gpt-4 as they use the function calling API from OpenAI.
condense_question - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine.
context - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.
condense_plus_context - A combination of condense_question and context. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.
simple - A simple chat with the LLM directly, no query engine involved.
react - Same as best, but forces a ReAct data agent.
openai - Same as best, but forces an OpenAI data agent.

Memory:
Chat Summary Memory Buffer
Mem0
Memory
      - Short term memory
      - Long term memory

ChatSummaryMemoryBuffer to limit the chat history to a certain token length, and iteratively summarize all messages that do not fit in the memory buffer. This can be useful if you want to limit costs and latency (assuming the summarization prompt uses and generates fewer tokens than including the entire history).
The original ChatMemoryBuffer gives you the option to truncate the history after a certain number of tokens, which is useful to limit costs and latency, but also removes potentially relevant information from the chat history.
The newer ChatSummaryMemoryBuffer aims to makes this a bit more flexible, so the user has more control over which chat_history is retained.

The Memory class in LlamaIndex is used to store and retrieve both short-term and long-term memory.
You can use it on its own and orchestrate within a custom workflow, or use it within an existing agent.
By default, short-term memory is represented as a FIFO queue of ChatMessage objects. Once the queue exceeds a certain size, the last X messages within a flush size are archived and optionally flushed to long-term memory blocks.
Long-term memory is represented as Memory Block objects. These objects receive the messages that are flushed from short-term memory, and optionally process them to extract information. Then when memory is retrieved, the short-term and long-term memories are merged together.
