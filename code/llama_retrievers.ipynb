{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divyakirant\\Documents\\llamaIndex\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_20792\\86501729.py:6: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
      "C:\\Users\\divyakirant\\AppData\\Local\\Temp\\ipykernel_20792\\86501729.py:7: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,Settings\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "\n",
    "# Configure Gemini models\n",
    "gemini_llm = Gemini(model=\"models/gemini-1.5-pro\")\n",
    "gemini_embedding = GeminiEmbedding(model=\"models/embedding-001\")\n",
    "\n",
    "# Tell LlamaIndex to use Gemini everywhere\n",
    "Settings.llm = gemini_llm\n",
    "Settings.embed_model = gemini_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader=SimpleDirectoryReader(input_dir=\"data\",required_exts=[\".txt\"])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node parser to split documents into smaller chunks\n",
    "# This is useful for large documents to ensure embeddings are manageable\n",
    "# Chunk size means the number of characters in each chunk\n",
    "# Chunk overlap means how many characters overlap between chunks, eg:\n",
    "# if chunk_size=100 and chunk_overlap=20, the first chunk will be characters 0-99,\n",
    "# the second chunk will be characters 80-179, and so on.\n",
    "# This helps maintain context between chunks.\n",
    "\n",
    "parser=SentenceSplitter(chunk_size=100,chunk_overlap=10)\n",
    "all_documents=parser.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors Store: <llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x000002D2D60670E0>\n",
      "Vectors Store ID: 1d2b2a40-6695-4c30-ae2f-c5f20e99b5d3\n"
     ]
    }
   ],
   "source": [
    "# store the documents in a vector store index\n",
    "index=VectorStoreIndex.from_documents(documents=all_documents)\n",
    "print(f\"Vectors Store: {index}\")\n",
    "print(f\"Vectors Store ID: {index.index_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "response_synthesizer=get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat engines use memory to create natural-flowing conversations, similar to human dialogue.  Memory is crucial because it provides context continuity (allowing for pronoun resolution), enables personalization, manages long conversations through summarization, and prevents exceeding token limits by controlling conversation history.  In query engines, memory helps refine queries over multiple turns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=query_engine.query(\"Give me the summary of this file\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
